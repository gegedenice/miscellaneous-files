{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb25c49-6b8a-4377-a9e5-eae1ea84f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U --quiet transformers huggingface_hub datasets bitsandbytes accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24152472-4e5c-4ef9-9384-c123d05216d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c --upgrade\n",
    "!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d07b6e-66c7-4320-af89-c7e11b5cff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "589cbd8a-20a9-475d-845b-706d7ba659a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0c29b35c9b4616ac97c67b91e977c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    #device_map=\"auto\",\n",
    "    #device_map={'':torch.cuda.current_device()},\n",
    "    #device_map={'': 'cuda:0'},\n",
    "    attn_implementation=\"eager\",\n",
    "    quantization_config=bnb_config\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1468f2f5-75ff-4994-a848-2953f057f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token if tokenizer.unk_token else tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right' # to prevent warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89bba0ce-ff91-4002-998e-c248b8edf712",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83209a8-61bd-4168-9942-ee4b619d8ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b136859546496890d24742262a48ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab730f823af04a80933fd7e6bfe2fd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e243cd2ff6b248da9ee0c9bac95c24b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/33300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Geraldine/Ead-Instruct-33k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91ace34-b367-4bda-becc-74158eb2cdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43f1e6c6f18490a93bb117a2dfb0ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_message = \"\"\"You are an archivist expert in EAD/XML format for archival records metadata.\"\"\"\n",
    "\n",
    "def create_conversation(row):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message},\n",
    "      {\"role\": \"user\", \"content\": row[\"prompt\"]},\n",
    "      {\"role\": \"assistant\", \"content\": row[\"completion\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bb3b8e-fe5c-415f-964e-ca0214953384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 33300\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be00baed-1c74-4824-89b8-825275f065c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are an archivist expert in EAD/XML format for archival records metadata.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Given this EAD/XML snippet representing a parent element, generate a valid child element that fits within the EAD structure. Snippet: <ead id=\"N65537\"></ead>',\n",
       "   'role': 'user'},\n",
       "  {'content': '<ead id=\"N65537\"><eadheader countryencoding=\"iso3166-1\" dateencoding=\"iso8601\" langencoding=\"iso639-2b\" relatedencoding=\"dc\" repositoryencoding=\"iso15511\" scriptencoding=\"iso15924\"><eadid countrycode=\"fr\" encodinganalog=\"identifier\" identifier=\"FRCGMNOV-330636101-jEG\" mainagencycode=\"751131015\">FRCGMNOV-330636101-jEG.xml</eadid></eadheader></ead>',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66f04dcb-3660-4b01-a488-e011ca6d96f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrldn-geoffroy\u001b[0m (\u001b[33mgrldn-geoffroy-smartbibl-ia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/wandb/run-20241224_115006-1l81qpas</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset/runs/1l81qpas' target=\"_blank\">earthy-blaze-1</a></strong> to <a href='https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset' target=\"_blank\">https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset/runs/1l81qpas' target=\"_blank\">https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset/runs/1l81qpas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"744a17e2b8e4079049d05739e13e7756d1f56e1a\")\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3.2 on EAD 33k dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "038a65fe-32fe-46f3-ae88-923481b7a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c356b0b-e1e5-4c6b-b7a1-9f123aeae6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs\", # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    #tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"wandb\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6150749-aba5-463e-9db1-f5615c7e7784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbb46143e8649929fceb512fc858b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (268674 > 131072). Running this sequence through the model will result in indexing errors\n",
      "/home/user/mambaforge/envs/env/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:305: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    },\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427d372b-5349-4404-9d70-435141cebcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1437' max='1437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1437/1437 1:46:10, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.871300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.438900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.475400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.519300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.424600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.349600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.293300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.351400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.194500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.202100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.214600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1437, training_loss=0.3322137685475784, metrics={'train_runtime': 6375.116, 'train_samples_per_second': 1.353, 'train_steps_per_second': 0.225, 'total_flos': 3.395386112905052e+17, 'train_loss': 0.3322137685475784, 'epoch': 2.994786235662148})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33572ab5-60d4-4011-bd71-969be143715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▄▁▂▂▄▁▄▂▂▄▁▃▁▂▄▂▂▆▅▃▄▄▆▂▃▅▃▄▃▂▂▇█▆▅▅▄▇▅▃</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▄▄▃▄▃▃▃▃▃▃▂▂▂▃▂▃▃▂▂▂▂▂▃▃▂▂▁▁▂▁▁▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>3.395386112905052e+17</td></tr><tr><td>train/epoch</td><td>2.99479</td></tr><tr><td>train/global_step</td><td>1437</td></tr><tr><td>train/grad_norm</td><td>0.10498</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.2146</td></tr><tr><td>train_loss</td><td>0.33221</td></tr><tr><td>train_runtime</td><td>6375.116</td></tr><tr><td>train_samples_per_second</td><td>1.353</td></tr><tr><td>train_steps_per_second</td><td>0.225</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-blaze-1</strong> at: <a href='https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset/runs/1l81qpas' target=\"_blank\">https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset/runs/1l81qpas</a><br> View project at: <a href='https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset' target=\"_blank\">https://wandb.ai/grldn-geoffroy-smartbibl-ia/Fine-tune%20Llama%203.2%20on%20EAD%2033k%20dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241224_115006-1l81qpas/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "009290f0-ea33-40a5-a559-89b12ddcfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "327e4eb1-d819-4e9d-8487-912e487f6b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6718c5e851b04bfc9ac4dff4c62f00e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/778M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Geraldine/outputs/commit/aecf4e83f4c8aa80dfff39c029352227eeb4e85d', commit_message='Upload model', commit_description='', oid='aecf4e83f4c8aa80dfff39c029352227eeb4e85d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Geraldine/outputs', endpoint='https://huggingface.co', repo_type='model', repo_id='Geraldine/outputs'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(\"outputs\", use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9fd5ffb-febc-4eb8-a0db-7c48682ecc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"outputs\"\n",
    "tr_model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5a94c83-b882-466d-8125-f30309f8cf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0473bedb89f64f9a996bc0874471d654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "model = AutoModelForCausalLM.from_pretrained(tr_model_id, trust_remote_code=True, torch_dtype=torch.float16,\n",
    "     low_cpu_mem_usage=True,)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "998a711e-1b39-414d-b125-be2da85f7ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9128d4094b943b09d3743a32813e039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0146cdee8354612b46596b73e6c3893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015dc3a814aa4508b9b7136dcc6e43fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Geraldine/FineLlama-3.2-3B-Instruct-ead/commit/4086d9bdb4a039fcba088f6b46204554208c846c', commit_message='Upload LlamaForCausalLM', commit_description='', oid='4086d9bdb4a039fcba088f6b46204554208c846c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Geraldine/FineLlama-3.2-3B-Instruct-ead', endpoint='https://huggingface.co', repo_type='model', repo_id='Geraldine/FineLlama-3.2-3B-Instruct-ead'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(f\"Geraldine/FineLlama-3.2-3B-Instruct-ead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7213e371-8bb6-43d3-9969-8fb727b19d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce088a9c464d429483eeb35cc2e2a297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2c8691c74d4c2fb6c9127f015fcd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Geraldine/FineLlama-3.2-3B-Instruct-ead/commit/432e22c0494879cca7312851e9700c87cc191da4', commit_message='Upload tokenizer', commit_description='', oid='432e22c0494879cca7312851e9700c87cc191da4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Geraldine/FineLlama-3.2-3B-Instruct-ead', endpoint='https://huggingface.co', repo_type='model', repo_id='Geraldine/FineLlama-3.2-3B-Instruct-ead'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "tokenizer.push_to_hub(f\"Geraldine/FineLlama-3.2-3B-Instruct-ead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a65e240-45bd-462f-bd89-e387777f4798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb87a10863d41adb221495703a69a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86387ffef2f34fb9ba0eaca905ae72ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4ffe55edd64cf59a5a4a686c5a2deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec48bc585b5746679d54d1c1b437731f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb12044bf04429b9721143a0b5e597f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545049b587f041a0aa066bba6a2203e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3912e01aa1548db9a55a7123f9a0c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812aea04e94447989ceaa1bcd3ebdb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060aee1eeed84f27a0c0827c0d11f6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc26c03e96c447118dce14171bebfe88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/439 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '<eadheader countryencoding=\"iso3166-1\" dateencoding=\"iso8601\" findaidstatus=\"provisoire\" langencoding=\"iso639-2b\" relatedencoding=\"dc\" repositoryencoding=\"iso15511\" scriptencoding=\"iso15924\"><filedesc><titlestmt><titleproper encodinganalog=\"title\">Bibliothèque scientifique du Muséum d\\'histoire naturelle de Nantes. Collection Voltaire (1694-1778)</titleproper><author encodinganalog=\"creator\">Bibliothèque scientifique du Muséum d\\'histoire naturelle de Nantes</author></titlestmt><publicationstmt><publisher encodinganalog=\"publisher\">Médiathèque Grand Nantes</publisher></publicationstmt></filedesc><profiledesc><langusage>Catalogue rédigé en</langusage></profiledesc></eadheader>'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"Geraldine/FineLlama-3.2-3B-Instruct-ead\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an archivist expert in EAD/XML format for archival records metadata. Structure every response as follows: Step 1 - Identify elements to generate: [List the XML elements needed] ; Step 2 - Generate each element: [Generate Element 1] [Generate Element 2] ... ; Step 3 - Merged result: [Complete XML with all elements properly nested]. Rules: - NEVER use ellipsis or abbreviations (...) ; - Generate complete content for each element ; - Ensure proper nesting and closing of all tags ; - Include all required attributes ; - Verify XML validity before providing final result\"},\n",
    "    {\"role\": \"user\", \"content\": \"I need a complete <eadheader> section\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=2048,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d1b74bd-d7a0-4d00-a61b-abac796ce1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '<archdesc level=\"fonds\"><did><repository><corpname authfilenumber=\"130016101\" normal=\"Bibliothèque municipale de Vannes\" source=\"Répertoire_des_Centres_de_Ressources\">Bibliothèque municipale de Vannes</corpname><address><addressline>Place de Bretagne</addressline><addressline>56000 Vannes</addressline><addressline>02.97.01.62.69</addressline></address></repository><unitid type=\"cotes_extrêmes\">1</unitid><unittitle>Fonds Martin Veretti</unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023/2023\">2023</unitdate></did><scopecontent><p>Le fonds Martin Veretti est composé de deux série de documents : une série de lettres et cartes postales reçues par Martin Veretti et une série de notes et d\\'extraits de textes.</p></scopecontent><bioghist><p> Martin Veretti est un historien et archiviste paléographe français. Né à Paris en 1957, il a commencé sa carrière professionnelle dans les collections patrimoniales de la bibliothèque nationale de France. En 1980, il devient conservateur-adjoint à la Bibliothèque nationale. En 1985, il est nommé conservateur de la Bibliothèque municipale de Vannes, charge qu\\'il occupe jusqu\\'en 1994. Pendant 9 ans, il est chargé de la collecte, du classement et de la conservation des manuscrits de cette bibliothèque. Pendant son séjour à Vannes, il a également animé plusieurs colloques et a organisé plusieurs expositions sur des sujets divers, notamment sur les collections patrimoniales et la vie de Jean Racine. En 1994, il est nommé conservateur de la Bibliothèque municipale de Rouen, charge qu\\'il occupe jusqu\\'en 2000. Pendant ses années passées à Rouen, il a animé plusieurs colloques et a organisé plusieurs expositions sur des sujets divers, notamment sur les collections patrimoniales et sur les artistes du XIXe siècle. En 2000, il est nommé conservateur de la Bibliothèque municipale de Vannes à nouveau. Il est chargé de la collecte, du classement et de la conservation des manuscrits et des documents imprimés de cette bibliothèque. Il est également chargé de l\\'animation des colloques et de l\\'organisation des expositions. En décembre 2022, il annonce sa candidature pour la succession du directeur général de la Bibliothèque nationale de France, François Moutet. </p></bioghist><acqinfo><p>Le fonds Martin Veretti a été créé en 2023 à l\\'occasion de la remise par la Bibliothèque municipale de Vannes des archives personnelles de Martin Veretti à cette bibliothèque.</p></acqinfo></c><c id=\"b2080347\" level=\"file\"><did><unitid type=\"cote\">MS FM 2023</unitid><unittitle> Lettres et cartes postales reçues par Martin Veretti </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 25 pièces de lettres et de cartes postales reçues par Martin Veretti et classées par ordre alphabétique des destinataires.</p></scopecontent></c><c id=\"b2080348\" level=\"file\"><did><unitid type=\"cote\">MS FM 2024</unitid><unittitle> Notes et extraits de textes reçus par Martin Veretti </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 35 pièces de notes et d\\'extraits de textes reçus par Martin Veretti et classés par ordre alphabétique des auteurs.</p></scopecontent></c></c><c id=\"b2080349\" level=\"file\"><did><unitid type=\"cote\">MS FM 2025</unitid><unittitle> Notes et extraits de textes reçus par Martin Veretti et correspondance professionnelle </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 50 pièces de notes et d\\'extraits de textes reçus par Martin Veretti et de correspondance professionnelle. Les pièces de correspondance professionnelle sont classées en sous-dossiers par ordre alphabétique des destinataires.</p></scopecontent></c></c><c id=\"b2080350\" level=\"file\"><did><unitid type=\"cote\">MS FM 2026</unitid><unittitle> Notes et extraits de textes reçus par Martin Veretti et correspondance professionnelle </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 55 pièces de notes et d\\'extraits de textes reçus par Martin Veretti et de correspondance professionnelle. Les pièces de correspondance professionnelle sont classées en sous-dossiers par ordre alphabétique des destinataires.</p></scopecontent></c><c id=\"b2080351\" level=\"file\"><did><unitid type=\"cote\">MS FM 2027</unitid><unittitle> Notes et extraits de textes reçus par Martin Veretti et correspondance professionnelle </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 60 pièces de notes et d\\'extraits de textes reçus par Martin Veretti et de correspondance professionnelle. Les pièces de correspondance professionnelle sont classées en sous-dossiers par ordre alphabétique des destinataires.</p></scopecontent></c></c></c><c id=\"b2080352\" level=\"file\"><did><unitid type=\"cote\">MS FM 2028</unitid><unittitle> Correspondance professionnelle reçue par Martin Veretti </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 30 pièces de correspondance professionnelle reçue par Martin Veretti.</p></scopecontent></c><c id=\"b2080353\" level=\"file\"><did><unitid type=\"cote\">MS FM 2029</unitid><unittitle> Correspondance professionnelle reçue par Martin Veretti </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 20 pièces de correspondance professionnelle reçue par Martin Veretti.</p></scopecontent></c></c></c><c id=\"b2080354\" level=\"file\"><did><unitid type=\"cote\">MS FM 2030</unitid><unittitle> Correspondance professionnelle reçue par Martin Veretti </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 20 pièces de correspondance professionnelle reçue par Martin Veretti.</p></scopecontent></c><c id=\"b2080355\" level=\"file\"><did><unitid type=\"cote\">MS FM 2031</unitid><unittitle> Correspondance professionnelle reçue par Martin Veretti </unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2023\">2023</unitdate><physdesc><extent>1 classeur à carton bleu</extent></physdesc></did><scopecontent><p>Le classeur est composé de 15 pièces de correspondance professionnelle reçue par Martin Veretti.</p></scopecontent></c><c id=\"b2080356\" level=\"file\"><did><unitid type=\"cote'}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an archivist expert in EAD/XML format for archival records metadata.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Create a complete EAD/XML template for the Veretti fund that has been created in 2023 and is composed of two main series : one serie of pieces of personal correspondance received by Martin Veretti and the other serie of personal notes.\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=2048,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6735e0a-3450-455e-be6b-29062102bf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ac2ced420c434a87ddcde621d018a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36670f036d542ada27449db3e4b4d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f973488225934204a2968f41d9ef5698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/user/FineLlama-3.2-3B-Instruct-ead'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"Geraldine/FineLlama-3.2-3B-Instruct-ead\", local_dir=\"FineLlama-3.2-3B-Instruct-ead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "892cb242-6ba3-483c-afa5-98645b13b702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/Geraldine/FineLlama-3.2-3B-Instruct-ead-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='Geraldine/FineLlama-3.2-3B-Instruct-ead-GGUF')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi(token=\"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\")\n",
    "\n",
    "model_id = \"Geraldine/FineLlama-3.2-3B-Instruct-ead-GGUF\"\n",
    "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fc9c507-cf61-4cb5-9170-fe1471bb03c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e4ff4ea6c94aa6a6b21dbc186f1a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FineLlama-3.2-3B-Instruct-ead-Q8_0.gguf:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Geraldine/FineLlama-3.2-3B-Instruct-ead-GGUF/commit/ef1ec2c1d179ce9018aecc384857b3d100165bb8', commit_message='Upload FineLlama-3.2-3B-Instruct-ead-Q8_0.gguf with huggingface_hub', commit_description='', oid='ef1ec2c1d179ce9018aecc384857b3d100165bb8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Geraldine/FineLlama-3.2-3B-Instruct-ead-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='Geraldine/FineLlama-3.2-3B-Instruct-ead-GGUF'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"FineLlama-3.2-3B-Instruct-ead-Q8_0.gguf\",\n",
    "    path_in_repo=\"FineLlama-3.2-3B-Instruct-ead-Q8_0.gguf\",\n",
    "    repo_id=model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34a3ab1d-b91d-448f-8720-897e4c29f93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795f07706fce44c2a97ec58d9f6b1dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Geraldine/FineLlama-3.2-3B-Instruct-ead\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    #device_map=\"auto\",\n",
    "    #device_map={'':torch.cuda.current_device()},\n",
    "    #device_map={'': 'cuda:0'},\n",
    "    attn_implementation=\"eager\",\n",
    "    quantization_config=bnb_config\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Geraldine/FineLlama-3.2-3B-Instruct-ead\")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an AI model expert in EAD/XML format for archival records metadata and specialized in generating EAD/XML content. \n",
    "You have been fine-tuned on instructions (prompt-completion pairs) to generate accurate and well-structured EAD/XML content based on user-provided data.\n",
    "\n",
    "### Instructions for Behavior:\n",
    "1. **Purpose:** Generate EAD/XML snippets strictly based on the information provided by the user. Do not assume or invent additional information beyond the given input.\n",
    "2. **Format:** Ensure all responses are structured in valid and well-formed EAD/XML format.\n",
    "3. **Boundaries:** If the user’s query requires information not explicitly provided in their input, respond with a clarification request, such as: *\"I can only generate based on the provided data. Could you provide more details?\"*\n",
    "4. **Error Handling:** If the provided input cannot be transformed into valid EAD/XML, explain why and provide guidance for correcting the input.\n",
    "5. **Conciseness:** Avoid adding any explanations, comments, or content outside of the EAD/XML snippet unless explicitly asked by the user.\n",
    "6. **Adherence to Standards:** Ensure the EAD/XML you generate adheres to the standard practices and structure of EAD (Encoded Archival Description).\n",
    "\n",
    "### Example Behavior:\n",
    "- **User Input:** \"Title: Inventory of the Smith Family Papers, Date: 2023, Language: English.\"\n",
    "- **Response:** \n",
    "```xml\n",
    "<ead>\n",
    "  <archdesc>\n",
    "    <did>\n",
    "      <unittitle>Inventory of the Smith Family Papers</unittitle>\n",
    "      <unitdate>2023</unitdate>\n",
    "      <langmaterial>\n",
    "        <language langcode=\"eng\">English</language>\n",
    "      </langmaterial>\n",
    "    </did>\n",
    "  </archdesc>\n",
    "</ead>\n",
    "```\n",
    "\n",
    "If you understand these instructions, you may begin responding to user queries.\n",
    "\n",
    "\"\"\"\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": system_prompt},\n",
    "  {\"role\": \"user\", \"content\": \"Generate a valid EAD template that describes the archival Fund Martin Veretti (which was the president of the EPFL from 2015 to 2022) : the Fonds Veretti has been created in 2023 by the EPFL library and it is composed of two main series : one serie of pieces of personal correspondance received by Martin Veretti and the other serie of personal notes.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    return_dict=True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 4096,pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68babdd8-ce88-41bd-bcc6-df5f94051554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Dec 2024\n",
      "\n",
      "You are an AI model expert in EAD/XML format for archival records metadata and specialized in generating EAD/XML content. \n",
      "You have been fine-tuned on instructions (prompt-completion pairs) to generate accurate and well-structured EAD/XML content based on user-provided data.\n",
      "\n",
      "### Instructions for Behavior:\n",
      "1. **Purpose:** Generate EAD/XML snippets strictly based on the information provided by the user. Do not assume or invent additional information beyond the given input.\n",
      "2. **Format:** Ensure all responses are structured in valid and well-formed EAD/XML format.\n",
      "3. **Boundaries:** If the user’s query requires information not explicitly provided in their input, respond with a clarification request, such as: *\"I can only generate based on the provided data. Could you provide more details?\"*\n",
      "4. **Error Handling:** If the provided input cannot be transformed into valid EAD/XML, explain why and provide guidance for correcting the input.\n",
      "5. **Conciseness:** Avoid adding any explanations, comments, or content outside of the EAD/XML snippet unless explicitly asked by the user.\n",
      "6. **Adherence to Standards:** Ensure the EAD/XML you generate adheres to the standard practices and structure of EAD (Encoded Archival Description).\n",
      "\n",
      "### Example Behavior:\n",
      "- **User Input:** \"Title: Inventory of the Smith Family Papers, Date: 2023, Language: English.\"\n",
      "- **Response:** \n",
      "```xml\n",
      "<ead>\n",
      "  <archdesc>\n",
      "    <did>\n",
      "      <unittitle>Inventory of the Smith Family Papers</unittitle>\n",
      "      <unitdate>2023</unitdate>\n",
      "      <langmaterial>\n",
      "        <language langcode=\"eng\">English</language>\n",
      "      </langmaterial>\n",
      "    </did>\n",
      "  </archdesc>\n",
      "</ead>\n",
      "```\n",
      "\n",
      "If you understand these instructions, you may begin responding to user queries.user\n",
      "\n",
      "Generate a valid EAD template that describes the archival Fund Martin Veretti (which was the president of the EPFL from 2015 to 2022) : the Fonds Veretti has been created in 2023 by the EPFL library and it is composed of two main series : one serie of pieces of personal correspondance received by Martin Veretti and the other serie of personal notes.assistant\n",
      "\n",
      "<ead>\n",
      "  <archdesc level=\"fonds\"><did><repository><corpname authfilenumber=\"https://catalogue.bnf.fr/ark:/12148/cb15340886m\" normal=\"Bibliothèque municipale (Lyon)\" source=\"BnF_catalogue_general\">Bibliothèque municipale de Lyon</corpname><address><addressline>30, rue de la Bibliothèque</addressline><addressline>69431 Lyon cedex 03</addressline><addressline>Tél. : 04.78.33.18.44</addressline></address></repository><unitid type=\"cote\">Fonds Martin Veretti</unitid><unittitle> Fonds <persname authfilenumber=\"https://catalogue.bnf.fr/ark:/12148/cb11915247g\" normal=\"Veretti, Martin\" role=\"sujet\" source=\"BnF_catalogue_general\">Martin Veretti</persname></unittitle><unitdate calendar=\"gregorian\" era=\"ce\" normal=\"2015/2023\">2015-2023</unitdate><langmaterial><language langcode=\"fre\">français</language></langmaterial><physdesc><physfacet type=\"support\">papier</physfacet></physdesc></did></archdesc></ead>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f99a4-6d2a-4e49-bf6f-72d2775f4e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
